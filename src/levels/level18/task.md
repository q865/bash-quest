### Дедовский Ликбез

Мы уже знаем, как скачать страницу (`curl`) и как найти в тексте нужную строку (`grep`). Пора, блядь, объединить эти два навыка, чтобы творить настоящую грязь.

Одна из самых частых задач парсинга — выдрать все ссылки со страницы. Ссылки в HTML живут внутри атрибута `href` у тега `<a>`. Выглядят они как-то так: `<a href="https://example.com/path">Click me</a>`.

Как нам их достать?
1.  Качаем страницу `curl`-ом.
2.  Результат по пайпу (`|`) отдаём `grep`-у.
3.  `grep`'ом ищем все строки, где есть `href=`.

Но `grep` вернёт всю строку, а нам нужна только сама ссылка. Для этого у `grep` есть охуенный ключ `-o` (only-matching), который показывает только ту часть, которая совпала с шаблоном. А чтобы задать сложный шаблон, используем ключ `-E` (extended regexp).

`curl -s example.com | grep -oE 'href="[^"]+"'`

Эта команда найдёт все `href="..."` и выплюнет их в консоль. А дальше уже можно их почистить от лишнего мусора.

### Твоя Задача

**Скачай `curl`-ом страницу `example.com`, вытащи из неё все ссылки (только `http` и `https`) и сохрани их в файл `links.txt` в папке `trainee_stuff`.**
