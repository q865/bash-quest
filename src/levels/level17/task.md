### Дедовский Ликбез

`curl` хорош, чтобы посмотреть на страницу или дёрнуть API. Но если тебе нужно скачать сайт целиком, со всеми картинками, стилями и скриптами, `curl`'ом ты заебёшься.

Для этого есть другой монстр — `wget`. Это, блядь, как пылесос для сайтов.

Самое простое — скачать одну страницу:
`wget example.com`
Он сохранит её в файл `index.html`.

Но настоящая магия — это рекурсивная загрузка. Ключ `-r` (recursive) или `--mirror` заставляет `wget` ходить по всем ссылкам на сайте и скачивать всё, что он найдёт. Так можно спиздить весь сайт целиком себе на комп для оффлайн-анализа.

`wget --mirror --convert-links --page-requisites --no-parent http://example.com`

Эта команда — джентльменский набор:
-   `--mirror`: Скачать всё нахуй.
-   `--convert-links`: Превратить ссылки в локальные, чтобы по сайту можно было ходить оффлайн.
-   `--page-requisites`: Скачать все картинки, CSS и прочую хуйню.
-   `--no-parent`: Не лезть на сайты выше уровнем (чтобы не скачать пол-интернета).

### Твоя Задача

Хватит теории.

**Используй `wget`, чтобы скачать главную страницу сайта `example.com`.**
